'''
webcrawler: (also known as a spider) is a system for downloading, storing, and analyzing web pages. 
It performs the task of organizing web pages so that users can easily find the information.

source: https://www.enjoyalgorithms.com/blog/web-crawler

REQUIREMENTS FOR THE WEB CRAWLER
1. Right Cadence
2. Duplicate Web Pages
3. Crawling Permission 

'''